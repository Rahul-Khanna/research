How To Run:

Third Iteration of Pipeline

1. python createReviewsToBeTagged.py
2. Manually annotate where the reviews should be segmented by editing the output of step 1
3. python tagger2.py
4. python trainTestSplitter.py
5. python scoringForSegmentation2.py
6. python crfForSegmentation.py
7. python windowDiff.py

Note:

- Steps 1-3 will not work unless you properly move over files from :
  https://github.com/cudbg/Dialectic

- To properly annotate the reviews you must choose a segmentation token, I have chosen "*|*|* ", and
  before the start of each new segment you insert this token.

  For example: This is segment1.*|*|* This is segment2!*|*|* Is this segment3?

- Given the data files in the data directory, steps 4-7 will work

Introduction:

Latent Dirichlet Allocation, LDA, is a generative statistical model that attempts to group words into topics given k, a number of topics, and a corpus. The framework of this model is that each document has a topic distribution, and each topic has a word distribution. The word distributions for each topic is a list a tuples that assigns a certain probability to each word in the corpus belonging to the topic. The idea behind the model is that each document is a sequence of words, and
to generate each word in a document, you first pick a topic from the available topics, and then from each topic you choose a word given the available words. The model learns the topic distribution and the word distributions that maximize the posterior likelihood of seeing the given corpus. The prior distributions of the word and topic distributions are Dirichlet.

For more information refer to this paper: https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf

Using a trained LDA model, https://radimrehurek.com/gensim/models/ldamodel.html, off of an Amazon Review corpus we wanted to look at if we could use topic distributions to train a model to segment text. The idea was that each segment would be dominated by one or two topics, and when a topic switch occurred that would mark the newest segment. The LDA model could be fed text, and would spit out the topics it thought were most present. The k, number of topics, being used was 18, so for each sentence of text we could associate a vector of length 18, where component i represented the likelihood that the sentence belonged to topic i. Then we could compare the distances between neighboring vectors and see if a change in distance resulted in a change of segment.

We decided to use a Linear Chain Conditional Random Field model in order to actually do the segmentation, as the model requires less training data, we had to go in and manually annote the Amazon reviews to mark segments, and due to the sequential nature of the data thought it could be a good fit. CRFs are much more flexible than HMMs, and allowed us to include observations into the model, instead of just focusing on likely transitions. We wanted the ability to say that a given a certain observation a certain labeling was much more likely, and not the other way around, as is the case with HMMs, given a label an observation is more likely. Essentially we wanted a classification task to be executed at each node...given the data classify the node as one of two possible labels:

0 - part of the current segment
1 - start of a new segment

We used this implementation of the CRF, pystruct.github.io

Results:

Our results weren't the best, and couldn't beat out the alternative of using Topic Tiling, https://ai2-s2-pdfs.s3.amazonaws.com/8ab0/c46b9b6cab8754ac7a130b822bad8e961b6f.pdf, but was a good excercise in understanding how CRFs work. Our window diff score wasn't better than Topic Tiling, but I would also say our ground truth wasn't a solid foundation, and this corpus was not the best to try and segment given the average length of Amazon Review and unorganized nature of the text. We also didn't fully understand how pystruct's implementation worked, so we were also using the model slightly wrong.


Moving Forward:

I still believe that CRFs can be used to segment larger pieces of text and am currently working on finding a good corpus to train on. I have also spoken to Andreas C. Mueller, one of the author's of pystruct, and have now developed a much better understanding of CRFs, and how pystruct's implementation works. Much more work will be put into generating the correct features to allow for proper classification at the node level.

I think CRFs are a really cool model due the ability to define classifiers at each node and use the labels of neighboring nodes to influence how the classification is done.


Understanding Code:

Each file is heavily commented, the following is just a brief summary of each file and what it does.

createReviewsToBeTagged.py:
Script that reads the raw Amazon Reviews, processes them, determines which of them were deemed helpful by the community, and then dices them up in different ways for further analysis.

tagger2.py:
Script that reads in the annotated text of the Amazon Reviews and then creates the Review objects for each text that are then used in all other scripts in this pipeline.

trainTestSplitter.py:
Script that splits the Review objects into testing and training sets. The size of the testing is 15% of the data

scoringForSegmentation2.py:
Script that generates the features for each sentence of each review in each data set.

crfForSegmentation.py:
Script that takes in the final data sets, and attempts to segment text based on the feature vectors associated with each sentence using a Linear Chain CRF model.

windowDiff.py:
Script that evaluates the window diff score of the segmented Reviews to be able to compare them to other segmentation methods.

Other files:

tagger.py:
A command line interface that breaks down reviews into sentences and allows a user to associate a tag with each sentence in each review.

scoringForSegmentation.py:
Script that generates the features for each sentence of each review, an older version of the pipeline used this.

dataExploration.py:
Script used to see if the actual distance feature vectors differed between transitions of the type (1-0 and 0-0) and (1-1 and 0-1). It turned out the difference was negligible, so our underlying hypothesis of new segments dealing with a different topic didn't hold... at least in this corpus(I really think its due to the nature of the corpus, than the underlying hypothesis being wrong).

visualizerOfSegmentedReviews.py:
Script that creates an html file that allows one to visualize the difference between the ground truth segmentation and the predicted segmentation. It's output can be found in the html folder.

visualizerTextToLda.py:
Script that creates an html file that allows one to visualize the feature vector generated for each sentence of each review. It's output can be found in the html folder.

modules/functions.py:
Module file that contains useful functions that are used in other scripts.

modules/reviewModules.py:
Module file that contains object definitions for objects pertaining to segmenting reviews.

modules/graphModules.py:
Module file that contains object definitions for another approach I tried based off of this paper, https://perso.limsi.fr/yvon/publications/sources/Misra09textsegmentation.pdf.




